{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f0dd1b7-47ac-4bab-b157-763c4a735b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce991c6-6ad0-47f0-93b4-49b5d551994c",
   "metadata": {},
   "source": [
    "# 1. Build and train a network to recognize digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c370574-7067-4ba9-9a65-baeca2c55704",
   "metadata": {},
   "source": [
    "## A. Get the MNIST digit data set\n",
    "The MNIST digit data consists of a training set of 60k 28x28 labeled digits and a test set of 10k 28x28 labeled digits. The data set can be imported directly from the torchvision package as torchvision.datasets.MNIST. Use the matplotlib pyplot package or OpenCV to look at the first six example digits. Look up examples that make use of the pyplot subplot method to create a grid of plots.\n",
    "\n",
    "Include a plot of the first six example digits in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1dbcef4a-052c-487f-9e97-e4a19045c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadData(is_train):\n",
    "    if(is_train):\n",
    "        batch_num = 64\n",
    "    else:\n",
    "        batch_num = 1000\n",
    "    \n",
    "     # 1. transform the training input data to tensor and normalize it\n",
    "    transform=torchvision.transforms.Compose(\n",
    "                [torchvision.transforms.ToTensor(),\n",
    "                 torchvision.transforms.Normalize(\n",
    "                    #   normalize with mean and std\n",
    "                    (0.1307,), (0.3801,)\n",
    "                )\n",
    "                ])\n",
    "    \n",
    "    \n",
    "    # 2. create a data loader\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            'mnist',\n",
    "            train=is_train,\n",
    "            download=True,\n",
    "            transform=transform),\n",
    "        batch_size=batch_num,\n",
    "        shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    # Enumerate on test loader will give me batch index \n",
    "    # the train loader itself returning the images(example_data) and the label for it (example_target)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a68280ab-d4ef-4623-b62a-eda0a7ebb673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data_loader) :\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # get the X and y\n",
    "    examples = enumerate(data_loader)\n",
    "    batch_idx, (sample_data, sample_targets) = next(examples)\n",
    "    print(\"examining train_data\")\n",
    "    \n",
    "    # print\n",
    "    print(batch_idx)\n",
    "    print(sample_data.shape)\n",
    "    print(sample_targets.shape)\n",
    "    \n",
    "    # plot the first 6 sample\n",
    "    fig = plt.figure()\n",
    "    for i in range(6):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(sample_data[i][0], cmap=\"gray\", interpolation=\"none\")\n",
    "        plt.title(\"Ground truth: %d\" %(sample_targets[i]))\n",
    "        plt.xticks([])\n",
    "                  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c50c09-20ce-4882-8128-ddc185643508",
   "metadata": {},
   "source": [
    "## C. Build a network model\n",
    "Similar to the example in the tutorial, create a network with the following layers.\n",
    "\n",
    "CNN\n",
    "- A convolution layer with 10 5x5 filters\n",
    "- A max pooling layer with a 2x2 window and a ReLU function applied.\n",
    "\n",
    "- A convolution layer with 20 5x5 filters\n",
    "- A dropout layer with a 0.5 dropout rate (50%)\n",
    "- A max pooling layer with a 2x2 window and a ReLU function applied\n",
    "\n",
    "RNN\n",
    "- A flattening operation followed by a fully connected Linear layer with 50 nodes and a ReLU function on the output\n",
    "- A final fully connected Linear layer with 10 nodes and the log_softmax function applied to the output.\n",
    "\n",
    "\n",
    "The design of your network class will be in the constructorand forward method of your Network class, derived from the torch nn.Module class.\n",
    "\n",
    "Put a diagram of your network in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7258366-0aab-4e0e-ae4a-8e9a06f491f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call the parent constructor\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        #1. CNN\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5) \n",
    "        \n",
    "        # final output : 20 Channels X 4 X 4\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d() # default is 0.5 or half\n",
    "        \n",
    "        #2. ANN\n",
    "        self.fc1 = nn.Linear(in_features=320, out_features=50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "     \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. first conv, max pool, relu\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # 2. 2nd conv, droptout layer, max pool, relu\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        \n",
    "        # 3. reshape tensor . why to -1, 320? this is same as flatten ?\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 4. fully connected, relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b7eac-7ae1-4689-9ce1-0d3f2a4a0aa3",
   "metadata": {},
   "source": [
    "## D. Train the network\n",
    "\n",
    "Train the model for five epochs, one epoch at a time, \n",
    "evaluating the model on both the training and test sets after each epoch. \n",
    "\n",
    "One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. \n",
    "\n",
    "Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.\n",
    "\n",
    "Pick a batch size of your choice. \n",
    "Make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cd363bc-74a4-4d59-9e1f-61977bc06cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train_loader, epoch_num):\n",
    "    # 1. train mode\n",
    "    network.train()\n",
    "    \n",
    "    # result to be plot\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    # For each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # 2. Manually set the gradients to zero using optimizer.zero_grad() since PyTorch by default accumulates gradients.\n",
    "        # use gradient descent\n",
    "        learning_rate = 0.1\n",
    "        momentum = 0.5\n",
    "        optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                              momentum=momentum)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "\n",
    "        # 3. compute a negative log-likelihodd (nll) loss between the output and the ground truth label\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. The backward() call we now collect a new set of gradients\n",
    "        # which we propagate back into each of the network's parameters using optimizer.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. what is log_interval?\n",
    "        print_interval = 10\n",
    "        if batch_idx % print_interval == 0:\n",
    "            # - print\n",
    "            print('Train Epoch: {}, batch index:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch_num,\n",
    "                batch_idx,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "            # - append result to plot\n",
    "            train_losses.append(loss.item()) # error value\n",
    "            train_counter.append( # number of samples the model has seen\n",
    "                (batch_idx*64) + ((epoch_num-1) * len(train_loader.dataset))\n",
    "            )\n",
    "\n",
    "            # - save network and optimizer\n",
    "            torch.save(network.state_dict(), 'results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n",
    "            \n",
    "    # return the error value for each batch and the number of samples\n",
    "    return train_counter, train_losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8c2f8-4fe4-4a63-80dc-c06a8d00e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, test_loader):\n",
    "      network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ff81e-7539-4db7-a619-be9fc575b1d8",
   "metadata": {},
   "source": [
    "## B. Make your network code reproducible\n",
    "In order to make your code reproducible, set the random seed for the torch package, torch.manual_seed(42), at the start of your main function. Remove this line if you want to create different networks. You can use any number you like, it doesn't have to be 42. To make your processing truly repeatable, you will also need to turn off CUDA using torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8793ec31-24ae-4b67-91ec-f2dec08b8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    # 1. make network reproducible\n",
    "    torch.manual_seed(42)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # 2. load and plot data\n",
    "    train_loader = loadData(is_train=True)\n",
    "    plotData(train_loader)\n",
    "    \n",
    "    # 3. create network model\n",
    "    network = NeuralNetwork()\n",
    "    \n",
    "    # 4. train network model\n",
    "    epoch_num = 5\n",
    "    \n",
    "    # create list to plot the number of training samples on x axis, and the scores\n",
    "    test_losses = []\n",
    "    test_counter = [i*len(train_loader.dataset) for i in range(epoch_num + 1)]\n",
    "    \n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        train_counter, train_losses = train_network(network, train_loader, epoch)\n",
    "        test_counter, test_losses = test_network(network, test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f254b8fb-400a-4620-a892-8b864d28a9bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining train_data\n",
      "0\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, batch index:0 [0/60000 (0%)]\tLoss: 2.310084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/26vt73dx7psbzwyygdm5wtt40000gn/T/ipykernel_8910/207760786.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, batch index:10 [640/60000 (1%)]\tLoss: 2.268983\n",
      "Train Epoch: 1, batch index:20 [1280/60000 (2%)]\tLoss: 2.213526\n",
      "Train Epoch: 1, batch index:30 [1920/60000 (3%)]\tLoss: 2.040806\n",
      "Train Epoch: 1, batch index:40 [2560/60000 (4%)]\tLoss: 1.538668\n",
      "Train Epoch: 1, batch index:50 [3200/60000 (5%)]\tLoss: 1.321530\n",
      "Train Epoch: 1, batch index:60 [3840/60000 (6%)]\tLoss: 1.038834\n",
      "Train Epoch: 1, batch index:70 [4480/60000 (7%)]\tLoss: 0.749378\n",
      "Train Epoch: 1, batch index:80 [5120/60000 (9%)]\tLoss: 1.210124\n",
      "Train Epoch: 1, batch index:90 [5760/60000 (10%)]\tLoss: 0.839677\n",
      "Train Epoch: 1, batch index:100 [6400/60000 (11%)]\tLoss: 0.837987\n",
      "Train Epoch: 1, batch index:110 [7040/60000 (12%)]\tLoss: 0.605440\n",
      "Train Epoch: 1, batch index:120 [7680/60000 (13%)]\tLoss: 0.759476\n",
      "Train Epoch: 1, batch index:130 [8320/60000 (14%)]\tLoss: 0.638498\n",
      "Train Epoch: 1, batch index:140 [8960/60000 (15%)]\tLoss: 0.556563\n",
      "Train Epoch: 1, batch index:150 [9600/60000 (16%)]\tLoss: 0.510298\n",
      "Train Epoch: 1, batch index:160 [10240/60000 (17%)]\tLoss: 1.031641\n",
      "Train Epoch: 1, batch index:170 [10880/60000 (18%)]\tLoss: 0.536259\n",
      "Train Epoch: 1, batch index:180 [11520/60000 (19%)]\tLoss: 1.003160\n",
      "Train Epoch: 1, batch index:190 [12160/60000 (20%)]\tLoss: 0.560060\n",
      "Train Epoch: 1, batch index:200 [12800/60000 (21%)]\tLoss: 0.564709\n",
      "Train Epoch: 1, batch index:210 [13440/60000 (22%)]\tLoss: 0.338193\n",
      "Train Epoch: 1, batch index:220 [14080/60000 (23%)]\tLoss: 0.582408\n",
      "Train Epoch: 1, batch index:230 [14720/60000 (25%)]\tLoss: 0.644776\n",
      "Train Epoch: 1, batch index:240 [15360/60000 (26%)]\tLoss: 0.578317\n",
      "Train Epoch: 1, batch index:250 [16000/60000 (27%)]\tLoss: 0.777394\n",
      "Train Epoch: 1, batch index:260 [16640/60000 (28%)]\tLoss: 0.705092\n",
      "Train Epoch: 1, batch index:270 [17280/60000 (29%)]\tLoss: 0.386994\n",
      "Train Epoch: 1, batch index:280 [17920/60000 (30%)]\tLoss: 0.436145\n",
      "Train Epoch: 1, batch index:290 [18560/60000 (31%)]\tLoss: 0.489519\n",
      "Train Epoch: 1, batch index:300 [19200/60000 (32%)]\tLoss: 0.815569\n",
      "Train Epoch: 1, batch index:310 [19840/60000 (33%)]\tLoss: 0.616435\n",
      "Train Epoch: 1, batch index:320 [20480/60000 (34%)]\tLoss: 0.244756\n",
      "Train Epoch: 1, batch index:330 [21120/60000 (35%)]\tLoss: 0.399254\n",
      "Train Epoch: 1, batch index:340 [21760/60000 (36%)]\tLoss: 0.194352\n",
      "Train Epoch: 1, batch index:350 [22400/60000 (37%)]\tLoss: 0.346828\n",
      "Train Epoch: 1, batch index:360 [23040/60000 (38%)]\tLoss: 0.499935\n",
      "Train Epoch: 1, batch index:370 [23680/60000 (39%)]\tLoss: 0.546195\n",
      "Train Epoch: 1, batch index:380 [24320/60000 (41%)]\tLoss: 0.282752\n",
      "Train Epoch: 1, batch index:390 [24960/60000 (42%)]\tLoss: 0.301170\n",
      "Train Epoch: 1, batch index:400 [25600/60000 (43%)]\tLoss: 0.366473\n",
      "Train Epoch: 1, batch index:410 [26240/60000 (44%)]\tLoss: 0.404671\n",
      "Train Epoch: 1, batch index:420 [26880/60000 (45%)]\tLoss: 0.517254\n",
      "Train Epoch: 1, batch index:430 [27520/60000 (46%)]\tLoss: 0.346201\n",
      "Train Epoch: 1, batch index:440 [28160/60000 (47%)]\tLoss: 0.500363\n",
      "Train Epoch: 1, batch index:450 [28800/60000 (48%)]\tLoss: 0.346406\n",
      "Train Epoch: 1, batch index:460 [29440/60000 (49%)]\tLoss: 0.441747\n",
      "Train Epoch: 1, batch index:470 [30080/60000 (50%)]\tLoss: 0.362010\n",
      "Train Epoch: 1, batch index:480 [30720/60000 (51%)]\tLoss: 0.405196\n",
      "Train Epoch: 1, batch index:490 [31360/60000 (52%)]\tLoss: 0.464455\n",
      "Train Epoch: 1, batch index:500 [32000/60000 (53%)]\tLoss: 0.372123\n",
      "Train Epoch: 1, batch index:510 [32640/60000 (54%)]\tLoss: 0.314489\n",
      "Train Epoch: 1, batch index:520 [33280/60000 (55%)]\tLoss: 0.398711\n",
      "Train Epoch: 1, batch index:530 [33920/60000 (57%)]\tLoss: 0.166304\n",
      "Train Epoch: 1, batch index:540 [34560/60000 (58%)]\tLoss: 0.379386\n",
      "Train Epoch: 1, batch index:550 [35200/60000 (59%)]\tLoss: 0.513473\n",
      "Train Epoch: 1, batch index:560 [35840/60000 (60%)]\tLoss: 0.418715\n",
      "Train Epoch: 1, batch index:570 [36480/60000 (61%)]\tLoss: 0.336469\n",
      "Train Epoch: 1, batch index:580 [37120/60000 (62%)]\tLoss: 0.389296\n",
      "Train Epoch: 1, batch index:590 [37760/60000 (63%)]\tLoss: 0.265835\n",
      "Train Epoch: 1, batch index:600 [38400/60000 (64%)]\tLoss: 0.318167\n",
      "Train Epoch: 1, batch index:610 [39040/60000 (65%)]\tLoss: 0.161476\n",
      "Train Epoch: 1, batch index:620 [39680/60000 (66%)]\tLoss: 0.442307\n",
      "Train Epoch: 1, batch index:630 [40320/60000 (67%)]\tLoss: 0.281051\n",
      "Train Epoch: 1, batch index:640 [40960/60000 (68%)]\tLoss: 0.252130\n",
      "Train Epoch: 1, batch index:650 [41600/60000 (69%)]\tLoss: 0.323089\n",
      "Train Epoch: 1, batch index:660 [42240/60000 (70%)]\tLoss: 0.217476\n",
      "Train Epoch: 1, batch index:670 [42880/60000 (71%)]\tLoss: 0.334731\n",
      "Train Epoch: 1, batch index:680 [43520/60000 (72%)]\tLoss: 0.317002\n",
      "Train Epoch: 1, batch index:690 [44160/60000 (74%)]\tLoss: 0.171697\n",
      "Train Epoch: 1, batch index:700 [44800/60000 (75%)]\tLoss: 0.277813\n",
      "Train Epoch: 1, batch index:710 [45440/60000 (76%)]\tLoss: 0.421116\n",
      "Train Epoch: 1, batch index:720 [46080/60000 (77%)]\tLoss: 0.277454\n",
      "Train Epoch: 1, batch index:730 [46720/60000 (78%)]\tLoss: 0.324130\n",
      "Train Epoch: 1, batch index:740 [47360/60000 (79%)]\tLoss: 0.357062\n",
      "Train Epoch: 1, batch index:750 [48000/60000 (80%)]\tLoss: 0.262581\n",
      "Train Epoch: 1, batch index:760 [48640/60000 (81%)]\tLoss: 0.279064\n",
      "Train Epoch: 1, batch index:770 [49280/60000 (82%)]\tLoss: 0.337258\n",
      "Train Epoch: 1, batch index:780 [49920/60000 (83%)]\tLoss: 0.154012\n",
      "Train Epoch: 1, batch index:790 [50560/60000 (84%)]\tLoss: 0.241484\n",
      "Train Epoch: 1, batch index:800 [51200/60000 (85%)]\tLoss: 0.482865\n",
      "Train Epoch: 1, batch index:810 [51840/60000 (86%)]\tLoss: 0.249083\n",
      "Train Epoch: 1, batch index:820 [52480/60000 (87%)]\tLoss: 0.081504\n",
      "Train Epoch: 1, batch index:830 [53120/60000 (88%)]\tLoss: 0.315582\n",
      "Train Epoch: 1, batch index:840 [53760/60000 (90%)]\tLoss: 0.194727\n",
      "Train Epoch: 1, batch index:850 [54400/60000 (91%)]\tLoss: 0.216895\n",
      "Train Epoch: 1, batch index:860 [55040/60000 (92%)]\tLoss: 0.397953\n",
      "Train Epoch: 1, batch index:870 [55680/60000 (93%)]\tLoss: 0.366485\n",
      "Train Epoch: 1, batch index:880 [56320/60000 (94%)]\tLoss: 0.408928\n",
      "Train Epoch: 1, batch index:890 [56960/60000 (95%)]\tLoss: 0.215707\n",
      "Train Epoch: 1, batch index:900 [57600/60000 (96%)]\tLoss: 0.323352\n",
      "Train Epoch: 1, batch index:910 [58240/60000 (97%)]\tLoss: 0.184490\n",
      "Train Epoch: 1, batch index:920 [58880/60000 (98%)]\tLoss: 0.146377\n",
      "Train Epoch: 1, batch index:930 [59520/60000 (99%)]\tLoss: 0.124457\n",
      "Train Epoch: 2, batch index:0 [0/60000 (0%)]\tLoss: 0.130886\n",
      "Train Epoch: 2, batch index:10 [640/60000 (1%)]\tLoss: 0.287996\n",
      "Train Epoch: 2, batch index:20 [1280/60000 (2%)]\tLoss: 0.279992\n",
      "Train Epoch: 2, batch index:30 [1920/60000 (3%)]\tLoss: 0.266695\n",
      "Train Epoch: 2, batch index:40 [2560/60000 (4%)]\tLoss: 0.214191\n",
      "Train Epoch: 2, batch index:50 [3200/60000 (5%)]\tLoss: 0.197226\n",
      "Train Epoch: 2, batch index:60 [3840/60000 (6%)]\tLoss: 0.149141\n",
      "Train Epoch: 2, batch index:70 [4480/60000 (7%)]\tLoss: 0.179143\n",
      "Train Epoch: 2, batch index:80 [5120/60000 (9%)]\tLoss: 0.303837\n",
      "Train Epoch: 2, batch index:90 [5760/60000 (10%)]\tLoss: 0.164828\n",
      "Train Epoch: 2, batch index:100 [6400/60000 (11%)]\tLoss: 0.141058\n",
      "Train Epoch: 2, batch index:110 [7040/60000 (12%)]\tLoss: 0.309624\n",
      "Train Epoch: 2, batch index:120 [7680/60000 (13%)]\tLoss: 0.242044\n",
      "Train Epoch: 2, batch index:130 [8320/60000 (14%)]\tLoss: 0.230566\n",
      "Train Epoch: 2, batch index:140 [8960/60000 (15%)]\tLoss: 0.268838\n",
      "Train Epoch: 2, batch index:150 [9600/60000 (16%)]\tLoss: 0.245785\n",
      "Train Epoch: 2, batch index:160 [10240/60000 (17%)]\tLoss: 0.340901\n",
      "Train Epoch: 2, batch index:170 [10880/60000 (18%)]\tLoss: 0.245634\n",
      "Train Epoch: 2, batch index:180 [11520/60000 (19%)]\tLoss: 0.329755\n",
      "Train Epoch: 2, batch index:190 [12160/60000 (20%)]\tLoss: 0.381659\n",
      "Train Epoch: 2, batch index:200 [12800/60000 (21%)]\tLoss: 0.257217\n",
      "Train Epoch: 2, batch index:210 [13440/60000 (22%)]\tLoss: 0.171442\n",
      "Train Epoch: 2, batch index:220 [14080/60000 (23%)]\tLoss: 0.217297\n",
      "Train Epoch: 2, batch index:230 [14720/60000 (25%)]\tLoss: 0.918728\n",
      "Train Epoch: 2, batch index:240 [15360/60000 (26%)]\tLoss: 0.604080\n",
      "Train Epoch: 2, batch index:250 [16000/60000 (27%)]\tLoss: 0.297397\n",
      "Train Epoch: 2, batch index:260 [16640/60000 (28%)]\tLoss: 0.381664\n",
      "Train Epoch: 2, batch index:270 [17280/60000 (29%)]\tLoss: 0.136425\n",
      "Train Epoch: 2, batch index:280 [17920/60000 (30%)]\tLoss: 0.149857\n",
      "Train Epoch: 2, batch index:290 [18560/60000 (31%)]\tLoss: 0.267080\n",
      "Train Epoch: 2, batch index:300 [19200/60000 (32%)]\tLoss: 0.491614\n",
      "Train Epoch: 2, batch index:310 [19840/60000 (33%)]\tLoss: 0.233705\n",
      "Train Epoch: 2, batch index:320 [20480/60000 (34%)]\tLoss: 0.154365\n",
      "Train Epoch: 2, batch index:330 [21120/60000 (35%)]\tLoss: 0.281232\n",
      "Train Epoch: 2, batch index:340 [21760/60000 (36%)]\tLoss: 0.094435\n",
      "Train Epoch: 2, batch index:350 [22400/60000 (37%)]\tLoss: 0.323752\n",
      "Train Epoch: 2, batch index:360 [23040/60000 (38%)]\tLoss: 0.303528\n",
      "Train Epoch: 2, batch index:370 [23680/60000 (39%)]\tLoss: 0.397864\n",
      "Train Epoch: 2, batch index:380 [24320/60000 (41%)]\tLoss: 0.150464\n",
      "Train Epoch: 2, batch index:390 [24960/60000 (42%)]\tLoss: 0.244480\n",
      "Train Epoch: 2, batch index:400 [25600/60000 (43%)]\tLoss: 0.125945\n",
      "Train Epoch: 2, batch index:410 [26240/60000 (44%)]\tLoss: 0.231035\n",
      "Train Epoch: 2, batch index:420 [26880/60000 (45%)]\tLoss: 0.246434\n",
      "Train Epoch: 2, batch index:430 [27520/60000 (46%)]\tLoss: 0.487000\n",
      "Train Epoch: 2, batch index:440 [28160/60000 (47%)]\tLoss: 0.315891\n",
      "Train Epoch: 2, batch index:450 [28800/60000 (48%)]\tLoss: 0.202156\n",
      "Train Epoch: 2, batch index:460 [29440/60000 (49%)]\tLoss: 0.318907\n",
      "Train Epoch: 2, batch index:470 [30080/60000 (50%)]\tLoss: 0.314017\n",
      "Train Epoch: 2, batch index:480 [30720/60000 (51%)]\tLoss: 0.281037\n",
      "Train Epoch: 2, batch index:490 [31360/60000 (52%)]\tLoss: 0.263534\n",
      "Train Epoch: 2, batch index:500 [32000/60000 (53%)]\tLoss: 0.217887\n",
      "Train Epoch: 2, batch index:510 [32640/60000 (54%)]\tLoss: 0.136296\n",
      "Train Epoch: 2, batch index:520 [33280/60000 (55%)]\tLoss: 0.129370\n",
      "Train Epoch: 2, batch index:530 [33920/60000 (57%)]\tLoss: 0.204551\n",
      "Train Epoch: 2, batch index:540 [34560/60000 (58%)]\tLoss: 0.123666\n",
      "Train Epoch: 2, batch index:550 [35200/60000 (59%)]\tLoss: 0.194174\n",
      "Train Epoch: 2, batch index:560 [35840/60000 (60%)]\tLoss: 0.338801\n",
      "Train Epoch: 2, batch index:570 [36480/60000 (61%)]\tLoss: 0.196792\n",
      "Train Epoch: 2, batch index:580 [37120/60000 (62%)]\tLoss: 0.277486\n",
      "Train Epoch: 2, batch index:590 [37760/60000 (63%)]\tLoss: 0.293248\n",
      "Train Epoch: 2, batch index:600 [38400/60000 (64%)]\tLoss: 0.277820\n",
      "Train Epoch: 2, batch index:610 [39040/60000 (65%)]\tLoss: 0.073815\n",
      "Train Epoch: 2, batch index:620 [39680/60000 (66%)]\tLoss: 0.281038\n",
      "Train Epoch: 2, batch index:630 [40320/60000 (67%)]\tLoss: 0.097257\n",
      "Train Epoch: 2, batch index:640 [40960/60000 (68%)]\tLoss: 0.282698\n",
      "Train Epoch: 2, batch index:650 [41600/60000 (69%)]\tLoss: 0.160344\n",
      "Train Epoch: 2, batch index:660 [42240/60000 (70%)]\tLoss: 0.123062\n",
      "Train Epoch: 2, batch index:670 [42880/60000 (71%)]\tLoss: 0.263845\n",
      "Train Epoch: 2, batch index:680 [43520/60000 (72%)]\tLoss: 0.201888\n",
      "Train Epoch: 2, batch index:690 [44160/60000 (74%)]\tLoss: 0.119292\n",
      "Train Epoch: 2, batch index:700 [44800/60000 (75%)]\tLoss: 0.230649\n",
      "Train Epoch: 2, batch index:710 [45440/60000 (76%)]\tLoss: 0.580090\n",
      "Train Epoch: 2, batch index:720 [46080/60000 (77%)]\tLoss: 0.275822\n",
      "Train Epoch: 2, batch index:730 [46720/60000 (78%)]\tLoss: 0.244926\n",
      "Train Epoch: 2, batch index:740 [47360/60000 (79%)]\tLoss: 0.244598\n",
      "Train Epoch: 2, batch index:750 [48000/60000 (80%)]\tLoss: 0.223859\n",
      "Train Epoch: 2, batch index:760 [48640/60000 (81%)]\tLoss: 0.193742\n",
      "Train Epoch: 2, batch index:770 [49280/60000 (82%)]\tLoss: 0.124981\n",
      "Train Epoch: 2, batch index:780 [49920/60000 (83%)]\tLoss: 0.121513\n",
      "Train Epoch: 2, batch index:790 [50560/60000 (84%)]\tLoss: 0.276282\n",
      "Train Epoch: 2, batch index:800 [51200/60000 (85%)]\tLoss: 0.688116\n",
      "Train Epoch: 2, batch index:810 [51840/60000 (86%)]\tLoss: 0.216140\n",
      "Train Epoch: 2, batch index:820 [52480/60000 (87%)]\tLoss: 0.167329\n",
      "Train Epoch: 2, batch index:830 [53120/60000 (88%)]\tLoss: 0.253331\n",
      "Train Epoch: 2, batch index:840 [53760/60000 (90%)]\tLoss: 0.200855\n",
      "Train Epoch: 2, batch index:850 [54400/60000 (91%)]\tLoss: 0.045903\n",
      "Train Epoch: 2, batch index:860 [55040/60000 (92%)]\tLoss: 0.235699\n",
      "Train Epoch: 2, batch index:870 [55680/60000 (93%)]\tLoss: 0.224636\n",
      "Train Epoch: 2, batch index:880 [56320/60000 (94%)]\tLoss: 0.198897\n",
      "Train Epoch: 2, batch index:890 [56960/60000 (95%)]\tLoss: 0.144660\n",
      "Train Epoch: 2, batch index:900 [57600/60000 (96%)]\tLoss: 0.211583\n",
      "Train Epoch: 2, batch index:910 [58240/60000 (97%)]\tLoss: 0.118626\n",
      "Train Epoch: 2, batch index:920 [58880/60000 (98%)]\tLoss: 0.083808\n",
      "Train Epoch: 2, batch index:930 [59520/60000 (99%)]\tLoss: 0.056186\n",
      "Train Epoch: 3, batch index:0 [0/60000 (0%)]\tLoss: 0.184294\n",
      "Train Epoch: 3, batch index:10 [640/60000 (1%)]\tLoss: 0.147904\n",
      "Train Epoch: 3, batch index:20 [1280/60000 (2%)]\tLoss: 0.189218\n",
      "Train Epoch: 3, batch index:30 [1920/60000 (3%)]\tLoss: 0.255219\n",
      "Train Epoch: 3, batch index:40 [2560/60000 (4%)]\tLoss: 0.192178\n",
      "Train Epoch: 3, batch index:50 [3200/60000 (5%)]\tLoss: 0.265249\n",
      "Train Epoch: 3, batch index:60 [3840/60000 (6%)]\tLoss: 0.165253\n",
      "Train Epoch: 3, batch index:70 [4480/60000 (7%)]\tLoss: 0.136362\n",
      "Train Epoch: 3, batch index:80 [5120/60000 (9%)]\tLoss: 0.540017\n",
      "Train Epoch: 3, batch index:90 [5760/60000 (10%)]\tLoss: 0.323267\n",
      "Train Epoch: 3, batch index:100 [6400/60000 (11%)]\tLoss: 0.121259\n",
      "Train Epoch: 3, batch index:110 [7040/60000 (12%)]\tLoss: 0.123131\n",
      "Train Epoch: 3, batch index:120 [7680/60000 (13%)]\tLoss: 0.214080\n",
      "Train Epoch: 3, batch index:130 [8320/60000 (14%)]\tLoss: 0.115575\n",
      "Train Epoch: 3, batch index:140 [8960/60000 (15%)]\tLoss: 0.273641\n",
      "Train Epoch: 3, batch index:150 [9600/60000 (16%)]\tLoss: 0.274039\n",
      "Train Epoch: 3, batch index:160 [10240/60000 (17%)]\tLoss: 0.698581\n",
      "Train Epoch: 3, batch index:170 [10880/60000 (18%)]\tLoss: 0.093971\n",
      "Train Epoch: 3, batch index:180 [11520/60000 (19%)]\tLoss: 0.299164\n",
      "Train Epoch: 3, batch index:190 [12160/60000 (20%)]\tLoss: 0.319438\n",
      "Train Epoch: 3, batch index:200 [12800/60000 (21%)]\tLoss: 0.153337\n",
      "Train Epoch: 3, batch index:210 [13440/60000 (22%)]\tLoss: 0.257396\n",
      "Train Epoch: 3, batch index:220 [14080/60000 (23%)]\tLoss: 0.220960\n",
      "Train Epoch: 3, batch index:230 [14720/60000 (25%)]\tLoss: 0.501828\n",
      "Train Epoch: 3, batch index:240 [15360/60000 (26%)]\tLoss: 0.196958\n",
      "Train Epoch: 3, batch index:250 [16000/60000 (27%)]\tLoss: 0.365616\n",
      "Train Epoch: 3, batch index:260 [16640/60000 (28%)]\tLoss: 0.403894\n",
      "Train Epoch: 3, batch index:270 [17280/60000 (29%)]\tLoss: 0.066777\n",
      "Train Epoch: 3, batch index:280 [17920/60000 (30%)]\tLoss: 0.139546\n",
      "Train Epoch: 3, batch index:290 [18560/60000 (31%)]\tLoss: 0.194485\n",
      "Train Epoch: 3, batch index:300 [19200/60000 (32%)]\tLoss: 0.255704\n",
      "Train Epoch: 3, batch index:310 [19840/60000 (33%)]\tLoss: 0.188731\n",
      "Train Epoch: 3, batch index:320 [20480/60000 (34%)]\tLoss: 0.050792\n",
      "Train Epoch: 3, batch index:330 [21120/60000 (35%)]\tLoss: 0.263660\n",
      "Train Epoch: 3, batch index:340 [21760/60000 (36%)]\tLoss: 0.091353\n",
      "Train Epoch: 3, batch index:350 [22400/60000 (37%)]\tLoss: 0.185275\n",
      "Train Epoch: 3, batch index:360 [23040/60000 (38%)]\tLoss: 0.231196\n",
      "Train Epoch: 3, batch index:370 [23680/60000 (39%)]\tLoss: 0.282745\n",
      "Train Epoch: 3, batch index:380 [24320/60000 (41%)]\tLoss: 0.091715\n",
      "Train Epoch: 3, batch index:390 [24960/60000 (42%)]\tLoss: 0.137003\n",
      "Train Epoch: 3, batch index:400 [25600/60000 (43%)]\tLoss: 0.091777\n",
      "Train Epoch: 3, batch index:410 [26240/60000 (44%)]\tLoss: 0.202574\n",
      "Train Epoch: 3, batch index:420 [26880/60000 (45%)]\tLoss: 0.177456\n",
      "Train Epoch: 3, batch index:430 [27520/60000 (46%)]\tLoss: 0.139833\n",
      "Train Epoch: 3, batch index:440 [28160/60000 (47%)]\tLoss: 0.286286\n",
      "Train Epoch: 3, batch index:450 [28800/60000 (48%)]\tLoss: 0.158705\n",
      "Train Epoch: 3, batch index:460 [29440/60000 (49%)]\tLoss: 0.178251\n",
      "Train Epoch: 3, batch index:470 [30080/60000 (50%)]\tLoss: 0.228302\n",
      "Train Epoch: 3, batch index:480 [30720/60000 (51%)]\tLoss: 0.125598\n",
      "Train Epoch: 3, batch index:490 [31360/60000 (52%)]\tLoss: 0.250575\n",
      "Train Epoch: 3, batch index:500 [32000/60000 (53%)]\tLoss: 0.223622\n",
      "Train Epoch: 3, batch index:510 [32640/60000 (54%)]\tLoss: 0.103486\n",
      "Train Epoch: 3, batch index:520 [33280/60000 (55%)]\tLoss: 0.172806\n",
      "Train Epoch: 3, batch index:530 [33920/60000 (57%)]\tLoss: 0.139535\n",
      "Train Epoch: 3, batch index:540 [34560/60000 (58%)]\tLoss: 0.097257\n",
      "Train Epoch: 3, batch index:550 [35200/60000 (59%)]\tLoss: 0.271620\n",
      "Train Epoch: 3, batch index:560 [35840/60000 (60%)]\tLoss: 0.293217\n",
      "Train Epoch: 3, batch index:570 [36480/60000 (61%)]\tLoss: 0.137835\n",
      "Train Epoch: 3, batch index:580 [37120/60000 (62%)]\tLoss: 0.293325\n",
      "Train Epoch: 3, batch index:590 [37760/60000 (63%)]\tLoss: 0.303186\n",
      "Train Epoch: 3, batch index:600 [38400/60000 (64%)]\tLoss: 0.138053\n",
      "Train Epoch: 3, batch index:610 [39040/60000 (65%)]\tLoss: 0.047127\n",
      "Train Epoch: 3, batch index:620 [39680/60000 (66%)]\tLoss: 0.170363\n",
      "Train Epoch: 3, batch index:630 [40320/60000 (67%)]\tLoss: 0.102492\n",
      "Train Epoch: 3, batch index:640 [40960/60000 (68%)]\tLoss: 0.170529\n",
      "Train Epoch: 3, batch index:650 [41600/60000 (69%)]\tLoss: 0.333268\n",
      "Train Epoch: 3, batch index:660 [42240/60000 (70%)]\tLoss: 0.097291\n",
      "Train Epoch: 3, batch index:670 [42880/60000 (71%)]\tLoss: 0.272844\n",
      "Train Epoch: 3, batch index:680 [43520/60000 (72%)]\tLoss: 0.270914\n",
      "Train Epoch: 3, batch index:690 [44160/60000 (74%)]\tLoss: 0.081998\n",
      "Train Epoch: 3, batch index:700 [44800/60000 (75%)]\tLoss: 0.184342\n",
      "Train Epoch: 3, batch index:710 [45440/60000 (76%)]\tLoss: 0.339699\n",
      "Train Epoch: 3, batch index:720 [46080/60000 (77%)]\tLoss: 0.216952\n",
      "Train Epoch: 3, batch index:730 [46720/60000 (78%)]\tLoss: 0.173304\n",
      "Train Epoch: 3, batch index:740 [47360/60000 (79%)]\tLoss: 0.346161\n",
      "Train Epoch: 3, batch index:750 [48000/60000 (80%)]\tLoss: 0.307435\n",
      "Train Epoch: 3, batch index:760 [48640/60000 (81%)]\tLoss: 0.112427\n",
      "Train Epoch: 3, batch index:770 [49280/60000 (82%)]\tLoss: 0.215533\n",
      "Train Epoch: 3, batch index:780 [49920/60000 (83%)]\tLoss: 0.140526\n",
      "Train Epoch: 3, batch index:790 [50560/60000 (84%)]\tLoss: 0.225827\n",
      "Train Epoch: 3, batch index:800 [51200/60000 (85%)]\tLoss: 0.311694\n",
      "Train Epoch: 3, batch index:810 [51840/60000 (86%)]\tLoss: 0.079442\n",
      "Train Epoch: 3, batch index:820 [52480/60000 (87%)]\tLoss: 0.080000\n",
      "Train Epoch: 3, batch index:830 [53120/60000 (88%)]\tLoss: 0.101441\n",
      "Train Epoch: 3, batch index:840 [53760/60000 (90%)]\tLoss: 0.074182\n",
      "Train Epoch: 3, batch index:850 [54400/60000 (91%)]\tLoss: 0.093600\n",
      "Train Epoch: 3, batch index:860 [55040/60000 (92%)]\tLoss: 0.061893\n",
      "Train Epoch: 3, batch index:870 [55680/60000 (93%)]\tLoss: 0.172363\n",
      "Train Epoch: 3, batch index:880 [56320/60000 (94%)]\tLoss: 0.459438\n",
      "Train Epoch: 3, batch index:890 [56960/60000 (95%)]\tLoss: 0.040716\n",
      "Train Epoch: 3, batch index:900 [57600/60000 (96%)]\tLoss: 0.578156\n",
      "Train Epoch: 3, batch index:910 [58240/60000 (97%)]\tLoss: 0.058429\n",
      "Train Epoch: 3, batch index:920 [58880/60000 (98%)]\tLoss: 0.060396\n",
      "Train Epoch: 3, batch index:930 [59520/60000 (99%)]\tLoss: 0.038206\n",
      "Train Epoch: 4, batch index:0 [0/60000 (0%)]\tLoss: 0.154652\n",
      "Train Epoch: 4, batch index:10 [640/60000 (1%)]\tLoss: 0.129608\n",
      "Train Epoch: 4, batch index:20 [1280/60000 (2%)]\tLoss: 0.212339\n",
      "Train Epoch: 4, batch index:30 [1920/60000 (3%)]\tLoss: 0.188841\n",
      "Train Epoch: 4, batch index:40 [2560/60000 (4%)]\tLoss: 0.163811\n",
      "Train Epoch: 4, batch index:50 [3200/60000 (5%)]\tLoss: 0.119459\n",
      "Train Epoch: 4, batch index:60 [3840/60000 (6%)]\tLoss: 0.065611\n",
      "Train Epoch: 4, batch index:70 [4480/60000 (7%)]\tLoss: 0.195139\n",
      "Train Epoch: 4, batch index:80 [5120/60000 (9%)]\tLoss: 0.162330\n",
      "Train Epoch: 4, batch index:90 [5760/60000 (10%)]\tLoss: 0.238349\n",
      "Train Epoch: 4, batch index:100 [6400/60000 (11%)]\tLoss: 0.343939\n",
      "Train Epoch: 4, batch index:110 [7040/60000 (12%)]\tLoss: 0.316758\n",
      "Train Epoch: 4, batch index:120 [7680/60000 (13%)]\tLoss: 0.094559\n",
      "Train Epoch: 4, batch index:130 [8320/60000 (14%)]\tLoss: 0.146295\n",
      "Train Epoch: 4, batch index:140 [8960/60000 (15%)]\tLoss: 0.210923\n",
      "Train Epoch: 4, batch index:150 [9600/60000 (16%)]\tLoss: 0.173370\n",
      "Train Epoch: 4, batch index:160 [10240/60000 (17%)]\tLoss: 0.358720\n",
      "Train Epoch: 4, batch index:170 [10880/60000 (18%)]\tLoss: 0.175185\n",
      "Train Epoch: 4, batch index:180 [11520/60000 (19%)]\tLoss: 0.264947\n",
      "Train Epoch: 4, batch index:190 [12160/60000 (20%)]\tLoss: 0.280130\n",
      "Train Epoch: 4, batch index:200 [12800/60000 (21%)]\tLoss: 0.133730\n",
      "Train Epoch: 4, batch index:210 [13440/60000 (22%)]\tLoss: 0.203949\n",
      "Train Epoch: 4, batch index:220 [14080/60000 (23%)]\tLoss: 0.241645\n",
      "Train Epoch: 4, batch index:230 [14720/60000 (25%)]\tLoss: 0.299899\n",
      "Train Epoch: 4, batch index:240 [15360/60000 (26%)]\tLoss: 0.152892\n",
      "Train Epoch: 4, batch index:250 [16000/60000 (27%)]\tLoss: 0.273751\n",
      "Train Epoch: 4, batch index:260 [16640/60000 (28%)]\tLoss: 0.232547\n",
      "Train Epoch: 4, batch index:270 [17280/60000 (29%)]\tLoss: 0.053307\n",
      "Train Epoch: 4, batch index:280 [17920/60000 (30%)]\tLoss: 0.160642\n",
      "Train Epoch: 4, batch index:290 [18560/60000 (31%)]\tLoss: 0.328750\n",
      "Train Epoch: 4, batch index:300 [19200/60000 (32%)]\tLoss: 0.320794\n",
      "Train Epoch: 4, batch index:310 [19840/60000 (33%)]\tLoss: 0.208923\n",
      "Train Epoch: 4, batch index:320 [20480/60000 (34%)]\tLoss: 0.101590\n",
      "Train Epoch: 4, batch index:330 [21120/60000 (35%)]\tLoss: 0.112569\n",
      "Train Epoch: 4, batch index:340 [21760/60000 (36%)]\tLoss: 0.085774\n",
      "Train Epoch: 4, batch index:350 [22400/60000 (37%)]\tLoss: 0.131605\n",
      "Train Epoch: 4, batch index:360 [23040/60000 (38%)]\tLoss: 0.324540\n",
      "Train Epoch: 4, batch index:370 [23680/60000 (39%)]\tLoss: 0.272230\n",
      "Train Epoch: 4, batch index:380 [24320/60000 (41%)]\tLoss: 0.063868\n",
      "Train Epoch: 4, batch index:390 [24960/60000 (42%)]\tLoss: 0.189632\n",
      "Train Epoch: 4, batch index:400 [25600/60000 (43%)]\tLoss: 0.141333\n",
      "Train Epoch: 4, batch index:410 [26240/60000 (44%)]\tLoss: 0.093237\n",
      "Train Epoch: 4, batch index:420 [26880/60000 (45%)]\tLoss: 0.222752\n",
      "Train Epoch: 4, batch index:430 [27520/60000 (46%)]\tLoss: 0.418882\n",
      "Train Epoch: 4, batch index:440 [28160/60000 (47%)]\tLoss: 0.147573\n",
      "Train Epoch: 4, batch index:450 [28800/60000 (48%)]\tLoss: 0.139750\n",
      "Train Epoch: 4, batch index:460 [29440/60000 (49%)]\tLoss: 0.222460\n",
      "Train Epoch: 4, batch index:470 [30080/60000 (50%)]\tLoss: 0.150828\n",
      "Train Epoch: 4, batch index:480 [30720/60000 (51%)]\tLoss: 0.268014\n",
      "Train Epoch: 4, batch index:490 [31360/60000 (52%)]\tLoss: 0.138583\n",
      "Train Epoch: 4, batch index:500 [32000/60000 (53%)]\tLoss: 0.190608\n",
      "Train Epoch: 4, batch index:510 [32640/60000 (54%)]\tLoss: 0.196955\n",
      "Train Epoch: 4, batch index:520 [33280/60000 (55%)]\tLoss: 0.199377\n",
      "Train Epoch: 4, batch index:530 [33920/60000 (57%)]\tLoss: 0.091860\n",
      "Train Epoch: 4, batch index:540 [34560/60000 (58%)]\tLoss: 0.108359\n",
      "Train Epoch: 4, batch index:550 [35200/60000 (59%)]\tLoss: 0.277220\n",
      "Train Epoch: 4, batch index:560 [35840/60000 (60%)]\tLoss: 0.190612\n",
      "Train Epoch: 4, batch index:570 [36480/60000 (61%)]\tLoss: 0.120658\n",
      "Train Epoch: 4, batch index:580 [37120/60000 (62%)]\tLoss: 0.137902\n",
      "Train Epoch: 4, batch index:590 [37760/60000 (63%)]\tLoss: 0.173535\n",
      "Train Epoch: 4, batch index:600 [38400/60000 (64%)]\tLoss: 0.202494\n",
      "Train Epoch: 4, batch index:610 [39040/60000 (65%)]\tLoss: 0.116570\n",
      "Train Epoch: 4, batch index:620 [39680/60000 (66%)]\tLoss: 0.116144\n",
      "Train Epoch: 4, batch index:630 [40320/60000 (67%)]\tLoss: 0.109303\n",
      "Train Epoch: 4, batch index:640 [40960/60000 (68%)]\tLoss: 0.209700\n",
      "Train Epoch: 4, batch index:650 [41600/60000 (69%)]\tLoss: 0.179275\n",
      "Train Epoch: 4, batch index:660 [42240/60000 (70%)]\tLoss: 0.125434\n",
      "Train Epoch: 4, batch index:670 [42880/60000 (71%)]\tLoss: 0.235675\n",
      "Train Epoch: 4, batch index:680 [43520/60000 (72%)]\tLoss: 0.203281\n",
      "Train Epoch: 4, batch index:690 [44160/60000 (74%)]\tLoss: 0.158349\n",
      "Train Epoch: 4, batch index:700 [44800/60000 (75%)]\tLoss: 0.122987\n",
      "Train Epoch: 4, batch index:710 [45440/60000 (76%)]\tLoss: 0.388197\n",
      "Train Epoch: 4, batch index:720 [46080/60000 (77%)]\tLoss: 0.314141\n",
      "Train Epoch: 4, batch index:730 [46720/60000 (78%)]\tLoss: 0.183788\n",
      "Train Epoch: 4, batch index:740 [47360/60000 (79%)]\tLoss: 0.402304\n",
      "Train Epoch: 4, batch index:750 [48000/60000 (80%)]\tLoss: 0.192562\n",
      "Train Epoch: 4, batch index:760 [48640/60000 (81%)]\tLoss: 0.122900\n",
      "Train Epoch: 4, batch index:770 [49280/60000 (82%)]\tLoss: 0.037890\n",
      "Train Epoch: 4, batch index:780 [49920/60000 (83%)]\tLoss: 0.152935\n",
      "Train Epoch: 4, batch index:790 [50560/60000 (84%)]\tLoss: 0.275546\n",
      "Train Epoch: 4, batch index:800 [51200/60000 (85%)]\tLoss: 0.365786\n",
      "Train Epoch: 4, batch index:810 [51840/60000 (86%)]\tLoss: 0.077141\n",
      "Train Epoch: 4, batch index:820 [52480/60000 (87%)]\tLoss: 0.073624\n",
      "Train Epoch: 4, batch index:830 [53120/60000 (88%)]\tLoss: 0.082847\n",
      "Train Epoch: 4, batch index:840 [53760/60000 (90%)]\tLoss: 0.057498\n",
      "Train Epoch: 4, batch index:850 [54400/60000 (91%)]\tLoss: 0.036067\n",
      "Train Epoch: 4, batch index:860 [55040/60000 (92%)]\tLoss: 0.131533\n",
      "Train Epoch: 4, batch index:870 [55680/60000 (93%)]\tLoss: 0.143420\n",
      "Train Epoch: 4, batch index:880 [56320/60000 (94%)]\tLoss: 0.150023\n",
      "Train Epoch: 4, batch index:890 [56960/60000 (95%)]\tLoss: 0.109158\n",
      "Train Epoch: 4, batch index:900 [57600/60000 (96%)]\tLoss: 0.309563\n",
      "Train Epoch: 4, batch index:910 [58240/60000 (97%)]\tLoss: 0.148351\n",
      "Train Epoch: 4, batch index:920 [58880/60000 (98%)]\tLoss: 0.150220\n",
      "Train Epoch: 4, batch index:930 [59520/60000 (99%)]\tLoss: 0.020163\n",
      "Train Epoch: 5, batch index:0 [0/60000 (0%)]\tLoss: 0.157212\n",
      "Train Epoch: 5, batch index:10 [640/60000 (1%)]\tLoss: 0.130815\n",
      "Train Epoch: 5, batch index:20 [1280/60000 (2%)]\tLoss: 0.218970\n",
      "Train Epoch: 5, batch index:30 [1920/60000 (3%)]\tLoss: 0.180706\n",
      "Train Epoch: 5, batch index:40 [2560/60000 (4%)]\tLoss: 0.319241\n",
      "Train Epoch: 5, batch index:50 [3200/60000 (5%)]\tLoss: 0.178860\n",
      "Train Epoch: 5, batch index:60 [3840/60000 (6%)]\tLoss: 0.092353\n",
      "Train Epoch: 5, batch index:70 [4480/60000 (7%)]\tLoss: 0.169333\n",
      "Train Epoch: 5, batch index:80 [5120/60000 (9%)]\tLoss: 0.273490\n",
      "Train Epoch: 5, batch index:90 [5760/60000 (10%)]\tLoss: 0.111020\n",
      "Train Epoch: 5, batch index:100 [6400/60000 (11%)]\tLoss: 0.448179\n",
      "Train Epoch: 5, batch index:110 [7040/60000 (12%)]\tLoss: 0.211781\n",
      "Train Epoch: 5, batch index:120 [7680/60000 (13%)]\tLoss: 0.176213\n",
      "Train Epoch: 5, batch index:130 [8320/60000 (14%)]\tLoss: 0.150851\n",
      "Train Epoch: 5, batch index:140 [8960/60000 (15%)]\tLoss: 0.153532\n",
      "Train Epoch: 5, batch index:150 [9600/60000 (16%)]\tLoss: 0.195160\n",
      "Train Epoch: 5, batch index:160 [10240/60000 (17%)]\tLoss: 0.479208\n",
      "Train Epoch: 5, batch index:170 [10880/60000 (18%)]\tLoss: 0.078693\n",
      "Train Epoch: 5, batch index:180 [11520/60000 (19%)]\tLoss: 0.225958\n",
      "Train Epoch: 5, batch index:190 [12160/60000 (20%)]\tLoss: 0.193724\n",
      "Train Epoch: 5, batch index:200 [12800/60000 (21%)]\tLoss: 0.105536\n",
      "Train Epoch: 5, batch index:210 [13440/60000 (22%)]\tLoss: 0.103882\n",
      "Train Epoch: 5, batch index:220 [14080/60000 (23%)]\tLoss: 0.198105\n",
      "Train Epoch: 5, batch index:230 [14720/60000 (25%)]\tLoss: 0.326679\n",
      "Train Epoch: 5, batch index:240 [15360/60000 (26%)]\tLoss: 0.199954\n",
      "Train Epoch: 5, batch index:250 [16000/60000 (27%)]\tLoss: 0.389582\n",
      "Train Epoch: 5, batch index:260 [16640/60000 (28%)]\tLoss: 0.112192\n",
      "Train Epoch: 5, batch index:270 [17280/60000 (29%)]\tLoss: 0.087745\n",
      "Train Epoch: 5, batch index:280 [17920/60000 (30%)]\tLoss: 0.098298\n",
      "Train Epoch: 5, batch index:290 [18560/60000 (31%)]\tLoss: 0.052753\n",
      "Train Epoch: 5, batch index:300 [19200/60000 (32%)]\tLoss: 0.234984\n",
      "Train Epoch: 5, batch index:310 [19840/60000 (33%)]\tLoss: 0.125545\n",
      "Train Epoch: 5, batch index:320 [20480/60000 (34%)]\tLoss: 0.064437\n",
      "Train Epoch: 5, batch index:330 [21120/60000 (35%)]\tLoss: 0.211867\n",
      "Train Epoch: 5, batch index:340 [21760/60000 (36%)]\tLoss: 0.060939\n",
      "Train Epoch: 5, batch index:350 [22400/60000 (37%)]\tLoss: 0.141213\n",
      "Train Epoch: 5, batch index:360 [23040/60000 (38%)]\tLoss: 0.287982\n",
      "Train Epoch: 5, batch index:370 [23680/60000 (39%)]\tLoss: 0.279474\n",
      "Train Epoch: 5, batch index:380 [24320/60000 (41%)]\tLoss: 0.064962\n",
      "Train Epoch: 5, batch index:390 [24960/60000 (42%)]\tLoss: 0.096801\n",
      "Train Epoch: 5, batch index:400 [25600/60000 (43%)]\tLoss: 0.136986\n",
      "Train Epoch: 5, batch index:410 [26240/60000 (44%)]\tLoss: 0.306733\n",
      "Train Epoch: 5, batch index:420 [26880/60000 (45%)]\tLoss: 0.141858\n",
      "Train Epoch: 5, batch index:430 [27520/60000 (46%)]\tLoss: 0.261594\n",
      "Train Epoch: 5, batch index:440 [28160/60000 (47%)]\tLoss: 0.176725\n",
      "Train Epoch: 5, batch index:450 [28800/60000 (48%)]\tLoss: 0.160354\n",
      "Train Epoch: 5, batch index:460 [29440/60000 (49%)]\tLoss: 0.221113\n",
      "Train Epoch: 5, batch index:470 [30080/60000 (50%)]\tLoss: 0.205726\n",
      "Train Epoch: 5, batch index:480 [30720/60000 (51%)]\tLoss: 0.226449\n",
      "Train Epoch: 5, batch index:490 [31360/60000 (52%)]\tLoss: 0.280631\n",
      "Train Epoch: 5, batch index:500 [32000/60000 (53%)]\tLoss: 0.190826\n",
      "Train Epoch: 5, batch index:510 [32640/60000 (54%)]\tLoss: 0.215385\n",
      "Train Epoch: 5, batch index:520 [33280/60000 (55%)]\tLoss: 0.393147\n",
      "Train Epoch: 5, batch index:530 [33920/60000 (57%)]\tLoss: 0.047061\n",
      "Train Epoch: 5, batch index:540 [34560/60000 (58%)]\tLoss: 0.075742\n",
      "Train Epoch: 5, batch index:550 [35200/60000 (59%)]\tLoss: 0.178300\n",
      "Train Epoch: 5, batch index:560 [35840/60000 (60%)]\tLoss: 0.119035\n",
      "Train Epoch: 5, batch index:570 [36480/60000 (61%)]\tLoss: 0.124794\n",
      "Train Epoch: 5, batch index:580 [37120/60000 (62%)]\tLoss: 0.241859\n",
      "Train Epoch: 5, batch index:590 [37760/60000 (63%)]\tLoss: 0.188278\n",
      "Train Epoch: 5, batch index:600 [38400/60000 (64%)]\tLoss: 0.238468\n",
      "Train Epoch: 5, batch index:610 [39040/60000 (65%)]\tLoss: 0.045596\n",
      "Train Epoch: 5, batch index:620 [39680/60000 (66%)]\tLoss: 0.120000\n",
      "Train Epoch: 5, batch index:630 [40320/60000 (67%)]\tLoss: 0.077859\n",
      "Train Epoch: 5, batch index:640 [40960/60000 (68%)]\tLoss: 0.159658\n",
      "Train Epoch: 5, batch index:650 [41600/60000 (69%)]\tLoss: 0.053063\n",
      "Train Epoch: 5, batch index:660 [42240/60000 (70%)]\tLoss: 0.155686\n",
      "Train Epoch: 5, batch index:670 [42880/60000 (71%)]\tLoss: 0.113886\n",
      "Train Epoch: 5, batch index:680 [43520/60000 (72%)]\tLoss: 0.345016\n",
      "Train Epoch: 5, batch index:690 [44160/60000 (74%)]\tLoss: 0.113003\n",
      "Train Epoch: 5, batch index:700 [44800/60000 (75%)]\tLoss: 0.151825\n",
      "Train Epoch: 5, batch index:710 [45440/60000 (76%)]\tLoss: 0.367653\n",
      "Train Epoch: 5, batch index:720 [46080/60000 (77%)]\tLoss: 0.243495\n",
      "Train Epoch: 5, batch index:730 [46720/60000 (78%)]\tLoss: 0.200974\n",
      "Train Epoch: 5, batch index:740 [47360/60000 (79%)]\tLoss: 0.233171\n",
      "Train Epoch: 5, batch index:750 [48000/60000 (80%)]\tLoss: 0.106160\n",
      "Train Epoch: 5, batch index:760 [48640/60000 (81%)]\tLoss: 0.090148\n",
      "Train Epoch: 5, batch index:770 [49280/60000 (82%)]\tLoss: 0.071410\n",
      "Train Epoch: 5, batch index:780 [49920/60000 (83%)]\tLoss: 0.102960\n",
      "Train Epoch: 5, batch index:790 [50560/60000 (84%)]\tLoss: 0.176782\n",
      "Train Epoch: 5, batch index:800 [51200/60000 (85%)]\tLoss: 0.171376\n",
      "Train Epoch: 5, batch index:810 [51840/60000 (86%)]\tLoss: 0.050149\n",
      "Train Epoch: 5, batch index:820 [52480/60000 (87%)]\tLoss: 0.066272\n",
      "Train Epoch: 5, batch index:830 [53120/60000 (88%)]\tLoss: 0.228241\n",
      "Train Epoch: 5, batch index:840 [53760/60000 (90%)]\tLoss: 0.094437\n",
      "Train Epoch: 5, batch index:850 [54400/60000 (91%)]\tLoss: 0.132259\n",
      "Train Epoch: 5, batch index:860 [55040/60000 (92%)]\tLoss: 0.094299\n",
      "Train Epoch: 5, batch index:870 [55680/60000 (93%)]\tLoss: 0.188597\n",
      "Train Epoch: 5, batch index:880 [56320/60000 (94%)]\tLoss: 0.113906\n",
      "Train Epoch: 5, batch index:890 [56960/60000 (95%)]\tLoss: 0.162900\n",
      "Train Epoch: 5, batch index:900 [57600/60000 (96%)]\tLoss: 0.056089\n",
      "Train Epoch: 5, batch index:910 [58240/60000 (97%)]\tLoss: 0.019953\n",
      "Train Epoch: 5, batch index:920 [58880/60000 (98%)]\tLoss: 0.020947\n",
      "Train Epoch: 5, batch index:930 [59520/60000 (99%)]\tLoss: 0.031016\n"
     ]
    }
   ],
   "source": [
    "main(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7667d-f968-4f55-8948-beb31c2ab8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50c667-4b46-45eb-b733-f5070ae381e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
